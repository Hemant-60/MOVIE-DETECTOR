{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Face detection model\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pickle\n",
    "\n",
    "#baseDirectory  = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "imageDirectory = os.path.dirname(\"DIRECTORY PATH FOR IMAGE DATASET\")\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('DIRECTORY PATH CONTAINING HAARCASCADE CLASSIFIERS/haarcascades/haarcascade_frontalface_alt2.xml')\n",
    "\n",
    "recognizer =cv2.face.LBPHFaceRecognizer_create() #cv2.face.creaeLBPHFaceRecognizer()#\n",
    "\n",
    "currentId = 0\n",
    "label_ids = {}\n",
    "yLabel = []\n",
    "xTrain = []\n",
    "        \n",
    "for root , dirs , files in os.walk(imageDirectory):\n",
    "    for file in files:\n",
    "        if file.endswith(\"png\") or file.endswith(\"jpg\"):\n",
    "            path = os.path.join(root,file)\n",
    "            label = os.path.basename(root).replace(\" \",\"-\").lower()\n",
    "            #os.path.dirname(path)  can be written as root\n",
    "            #print(label,path)\n",
    "\n",
    "            if label in label_ids:\n",
    "                pass\n",
    "            else:\n",
    "                label_ids[label] = currentId\n",
    "                currentId+=1\n",
    "            id_=label_ids[label]\n",
    "            #print(label_ids)\n",
    "\n",
    "           # yLabel.append(label)\n",
    "            #xTrain.append(path)\n",
    "            pilImage = Image.open(path).convert(\"L\")\n",
    "            size = (550, 550)\n",
    "            final_image = pilImage.resize(size,Image.ANTIALIAS)\n",
    "            imageArray = np.array(pilImage, \"uint8\")\n",
    "            #print(imageArray)\n",
    "            faces = face_cascade.detectMultiScale(imageArray, scaleFactor=1.5  ,minNeighbors = 5)\n",
    "\n",
    "            #print(id_)\n",
    "            for (x,y,w,h) in faces:\n",
    "                roi = imageArray[y:y+h , x:x+h]\n",
    "                xTrain.append(roi)\n",
    "                yLabel.append(id_)\n",
    "\n",
    "# print(yLabel)\n",
    "# print(xTrain)\n",
    "\n",
    "with open(\"labels.pickle\", 'wb') as f:\n",
    "    pickle.dump(label_ids , f)\n",
    "\n",
    "recognizer.train(xTrain , np.array(yLabel))\n",
    "recognizer.save(\"trainner.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tomcruise']\n"
     ]
    }
   ],
   "source": [
    "#Recognizing faces in a clip\n",
    "\n",
    "import numpy as np \n",
    "import cv2\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "eye_cascade = cv2.CascadeClassifier('DIRECTORY PATH CONTAINING HAARCASCADE CLASSIFIERS/haarcascades/haarcascade_eye.xml')\n",
    "face_cascade = cv2.CascadeClassifier('DIRECTORY PATH CONTAINING HAARCASCADE CLASSIFIERS/haarcascades/haarcascade_frontalface_alt2.xml')\n",
    "recognizer =cv2.face.LBPHFaceRecognizer_create() #cv2.face.createLBPHFaceRecognizer()#\n",
    "smile_cascade = cv2.CascadeClassifier('DIRECTORY PATH CONTAINING HAARCASCADE CLASSIFIERS/haarcascades/haarcascade_smile.xml')\n",
    "\n",
    "actors=[]\n",
    "\n",
    "s=0 #timer variable\n",
    "#--------------Finding the length of the video in seconds--------------------------------\n",
    "# cap.set(cv2.CAP_PROP_POS_AVI_RATIO,1)\n",
    "# length=cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "# length=str(math.ceil(length))[0:3]\n",
    "# length=int(length)\n",
    "#print(length)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "recognizer.read(\"trainner.yml\")\n",
    "labels={\"person_name\": 1}\n",
    "\n",
    "with open(\"labels.pickle\", 'rb') as f:\n",
    "    ogLabels = pickle.load(f)\n",
    "    labels = {v:k for k,v in ogLabels.items()}\n",
    "\n",
    "cap=cv2.VideoCapture(\"PATH OF THE TEST VIDEO FILE\")\n",
    "#cap =cv2.VideoCapture(\"C:/Users/heman/Downloads/Mission.Impossible.Fallout.2018.720p.HC.HDRip.HEVC.MkvCage.mkv\")\n",
    "\n",
    "total_frames=cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "#print(total_frames)\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    current_frame=cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(gray , scaleFactor=1.5  ,minNeighbors = 5)\n",
    "    for(x, y, w, h) in faces:\n",
    "        #print(x,y,w,h)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "        id_ , conf = recognizer.predict(roi_gray)\n",
    "        #print(conf)\n",
    "        #print(conf,\" \",labels[id_])\n",
    "        if(labels[id_] not in actors ):\n",
    "            actors.append(labels[id_])\n",
    "        if (conf >= 4) and (conf <= 85):\n",
    "            #print(id_)\n",
    "            #print(labels[id_])\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            name = labels[id_]\n",
    "            color=(255,255,255)\n",
    "            stroke= 2\n",
    "            cv2.putText(frame,name,(x,y), font ,1 ,color ,stroke ,cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        img_item = \"7.png\"\n",
    "        cv2.imwrite(img_item,roi_gray)\n",
    "        color = (72,120 ,0)\n",
    "        stroke = 4\n",
    "        cv2.rectangle(frame, (x,y) , (x+w, y+h),color , stroke) \n",
    "        subitems = smile_cascade.detectMultiScale(roi_gray)\n",
    "        for(ex,ey,ew,eh) in subitems:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew , ey+eh) , (8,255,8) ,2)\n",
    "\n",
    "\n",
    "    cv2.imshow('frame' ,frame)\n",
    "    #cap.get(cv2.cv.CV_CAP_PROP_POS_FRAMES)\n",
    "    if cv2.waitKey(20) & 0xFF ==ord('q'):\n",
    "        break\n",
    "    #print(current_frame)\n",
    "    if (current_frame>=(total_frames-1)):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#roi  region of interest\n",
    "\n",
    "print(actors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in Intro.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "# AUDIO TO VIDEO CONVERSION\n",
    "\n",
    "import moviepy.editor as mp\n",
    "clip = mp.VideoFileClip(\"PATH OF THE TEST VIDEO FILE\")\n",
    "clip.audio.write_audiofile(\"NAME OF THE OUTPUT AUDIO FILE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"final\": true,\n",
      "      \"alternatives\": [\n",
      "        {\n",
      "          \"transcript\": \"fate whispers to the warrior \",\n",
      "          \"confidence\": 0.74\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"final\": true,\n",
      "      \"alternatives\": [\n",
      "        {\n",
      "          \"transcript\": \"storms coming \",\n",
      "          \"confidence\": 0.66\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"final\": true,\n",
      "      \"alternatives\": [\n",
      "        {\n",
      "          \"transcript\": \"and the warrior whispers back \",\n",
      "          \"confidence\": 0.7\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"final\": true,\n",
      "      \"alternatives\": [\n",
      "        {\n",
      "          \"transcript\": \"I am the star \",\n",
      "          \"confidence\": 0.48\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"final\": true,\n",
      "      \"alternatives\": [\n",
      "        {\n",
      "          \"transcript\": \"%HESITATION \",\n",
      "          \"confidence\": 0.99\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"final\": true,\n",
      "      \"alternatives\": [\n",
      "        {\n",
      "          \"transcript\": \"good evening Mr out the anarchist Solomon like \",\n",
      "          \"confidence\": 0.66\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"final\": true,\n",
      "      \"alternatives\": [\n",
      "        {\n",
      "          \"transcript\": \"since you captured him to \",\n",
      "          \"confidence\": 0.83\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"result_index\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# USING IBM WATSON FOR AUDIO TO TEXT CONVERSION\n",
    "\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_watson.websocket import RecognizeCallback, AudioSource\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "\n",
    "authenticator = IAMAuthenticator('API KEY')\n",
    "speech_to_text = SpeechToTextV1(\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "speech_to_text.set_service_url('https://gateway-lon.watsonplatform.net/speech-to-text/api')\n",
    "\n",
    "class MyRecognizeCallback(RecognizeCallback):\n",
    "    def __init__(self):\n",
    "        RecognizeCallback.__init__(self)\n",
    "\n",
    "    def on_data(self, data):\n",
    "        print(json.dumps(data, indent=2))\n",
    "        with open(\"FILE NAME IN WHICH OUTPUT WILL BE WRITTEN.json\", \"w+\") as read_file:\n",
    "            json.dump(data,read_file)\n",
    "        \n",
    "    def on_error(self, error):\n",
    "        print('Error received: {}'.format(error))\n",
    "\n",
    "    def on_inactivity_timeout(self, error):\n",
    "        print('Inactivity timeout: {}'.format(error))\n",
    "\n",
    "myRecognizeCallback = MyRecognizeCallback()\n",
    "\n",
    "with open(join(dirname('conv'), './.', 'AUDIO FILE PATH'),\n",
    "              'rb') as audio_file:\n",
    "    audio_source = AudioSource(audio_file)\n",
    "    speech_to_text.recognize_using_websocket(\n",
    "        audio=audio_source,\n",
    "        content_type='audio/mp3',\n",
    "        recognize_callback=myRecognizeCallback,\n",
    "        model='en-US_BroadbandModel',\n",
    "        max_alternatives=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fate whispers to the warrior ', 'storms coming ', 'and the warrior whispers back ', 'I am the star ', '%HESITATION ', 'good evening Mr out the anarchist Solomon like ', 'since you captured him to ']\n"
     ]
    }
   ],
   "source": [
    "# FETCHING ONLY DIALOGS FROM THE OUTPUT\n",
    "import json\n",
    "file_path=\"FILE NAME TO WHICH RESULTS WERE WRITTEN.json\"\n",
    "\n",
    "\n",
    "with open(file_path, \"r\") as read_file:\n",
    "    data = json.load(read_file)\n",
    "\n",
    "fetched_dialogs=[]\n",
    "for i in range(0,len(data[\"results\"])):\n",
    "    fetched_dialogs.append(data[\"results\"][i][\"alternatives\"][0][\"transcript\"])\n",
    "    \n",
    "print(fetched_dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9148936170212766\n"
     ]
    }
   ],
   "source": [
    "#COMPARING FETCHED DIALOGS FROM THE SOURCE WITH THE DIALOGS STORED IN OUR DATABASE\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "\n",
    "print(similar(fetched_dialogs[5],'good evening Mr hunt the anarchist Solomon lane'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
